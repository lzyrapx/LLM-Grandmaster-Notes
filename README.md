# LLM Grandmaster Notes

ðŸ“šThe path to LLM mastery is paved with broken embeddings and resurrected gradients.

- base
  + [x] lm head
  + [ ] kv cache
- attention
  + [x] self attention
  + [x] online attention
  + [x] flash attention
  + [x] flash attention 2
  + [x] flash attention 3
  + [x] flash decoding
  + [x] flash decoding++
  + [ ] multi-head attention (MHA)
  + [ ] grouped-query attention (GQA)
  + [ ] multi-query attention (MQA)
  + [ ] multi-head latent attention (MLA)
  + [ ] multi-token attention (MTA)
  + [ ] sage attention 1
  + [ ] sage attention 2
  + [ ] sage attention 3
  + [ ] paged attention
  + [ ] ring attention
  + [ ] linear attention
  + [ ] lightning attention
  + [ ] native sparse attention (NSA)
  + [ ] grouped latent attention (GLA)
  + [ ] grouped-tied attention (GTA)
- softmax
  + [x] softmax
  + [x] safe softmax
  + [x] online softmax
- kv cache optimization
  + [ ] sparse
  + [ ] quantization
  + [ ] allocator
  + [ ] window
  + [ ] share
- norm
  + [ ] Layer Norm
  + [ ] RMS Norm
  + [ ] Batch Norm
- position embedding
  + [ ] RoPE
  + [ ] AliBi
  + [ ] 2D RoPE
  + [ ] 3D RoPE
  + [ ] NTK-Award RoPE
  + [ ] Yarn
- quantization
  + [ ] smooth quant
  + [ ] AWQ
  + [ ] KIVI
- design
  + [ ] chunked prefill
  + [ ] continous batching
  + [ ] speculative decoding
  + [ ] sliding window
  + [ ] Multi-Token Prediction (MTP)
- reinforcement learning

