# LLM Grandmaster Notes

ðŸ“šThe path to LLM mastery is paved with broken embeddings and resurrected gradients.

- base
  + [x] lm head
- attention
  + [x] self attention
  + [x] online attention
  + [x] flash attention
  + [x] flash attention 2
  + [x] flash attention 3
  + [ ] multi-head attention (MHA)
  + [ ] grouped-query attention (GQA)
  + [ ] multi-query attention (MQA)
  + [ ] multi-head latent attention (MLA)
  + [ ] sage attention 1
  + [ ] sage attention 2
  + [ ] paged attention
- softmax
  + [x] softmax
  + [x] safe softmax
- norm
  + [ ] Layer Norm
  + [ ] RMS Norm
- position embedding
  + [ ] RoPE
  + [ ] AliBi
  + [ ] 2D RoPE
  + [ ] 3D RoPE
- quantization
  + [ ] SmoothQuant
  + [ ] AWQ
  

